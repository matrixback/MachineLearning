# 第8章 预测数值型数据：回归

分类：标称型数据。

回归：连续型数据。

“回归（regression）”的来历：1877年，达尔文的兄弟，根据豌豆双亲的尺寸来预测下一代的尺寸。发现如果双亲的高度比平均高度高，则他们的子女也倾向于比平均高度到，但是尚不及双亲。孩子的高度向着平均高度“回退”。


## 8.1 用线性回归找到最佳拟合直线

回归一般指线性回归：将输入项分布乘以一些常量，再将结果加起来得到输出。

如果数据集 x, 对应的结果集为 y，假设系数为 w，那么根据系数计算出来的结果集为 xTw，最好的 w，即为

y - xTw 的误差最小。使用平方误差，则得到 sum(pow((yi - xiTw), 2))。用矩阵表示为(Y-Xw)T(Y-Xw)。对
w 求导，得到 xT(Y-Xw），令其为0，解的 w = 1/(XTX)XTy。

注意，XTX 需要求逆，必须先判断是否可逆才行。

该方法叫做 OLS（ordinary least squares)。

最小二乘法的思路历程：线性回归 -->  平方误差最小 --> 正好可以求导，令导数为0 --> 得到结果集。


任一数据集都可以用上述方法建立模型，其为一条直线。如果多个图的拟合直线一致，对于每个图来说，直线是最优的。
但是，多个图之间怎么比较？

还是用平方误差，看差多少？

可以用相关系数的计算方法，计算预测值与真实值的相关性。

## 8.2 局部加权线性回归

线性回归中，有可能出现欠拟合现象。因为其求的是最小均方误差的无偏估计。

局部线性回归（Locally Weighted Linear Regression）：先划分子集，然后在子集上用最小二乘。





